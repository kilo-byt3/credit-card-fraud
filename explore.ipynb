{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r'.\\data\\train.csv')\n",
    "test = pd.read_csv(r'.\\data\\test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 219129 entries, 0 to 219128\n",
      "Data columns (total 32 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   id      219129 non-null  int64  \n",
      " 1   Time    219129 non-null  float64\n",
      " 2   V1      219129 non-null  float64\n",
      " 3   V2      219129 non-null  float64\n",
      " 4   V3      219129 non-null  float64\n",
      " 5   V4      219129 non-null  float64\n",
      " 6   V5      219129 non-null  float64\n",
      " 7   V6      219129 non-null  float64\n",
      " 8   V7      219129 non-null  float64\n",
      " 9   V8      219129 non-null  float64\n",
      " 10  V9      219129 non-null  float64\n",
      " 11  V10     219129 non-null  float64\n",
      " 12  V11     219129 non-null  float64\n",
      " 13  V12     219129 non-null  float64\n",
      " 14  V13     219129 non-null  float64\n",
      " 15  V14     219129 non-null  float64\n",
      " 16  V15     219129 non-null  float64\n",
      " 17  V16     219129 non-null  float64\n",
      " 18  V17     219129 non-null  float64\n",
      " 19  V18     219129 non-null  float64\n",
      " 20  V19     219129 non-null  float64\n",
      " 21  V20     219129 non-null  float64\n",
      " 22  V21     219129 non-null  float64\n",
      " 23  V22     219129 non-null  float64\n",
      " 24  V23     219129 non-null  float64\n",
      " 25  V24     219129 non-null  float64\n",
      " 26  V25     219129 non-null  float64\n",
      " 27  V26     219129 non-null  float64\n",
      " 28  V27     219129 non-null  float64\n",
      " 29  V28     219129 non-null  float64\n",
      " 30  Amount  219129 non-null  float64\n",
      " 31  Class   219129 non-null  int64  \n",
      "dtypes: float64(30), int64(2)\n",
      "memory usage: 53.5 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>219129.000000</td>\n",
       "      <td>219129.000000</td>\n",
       "      <td>219129.000000</td>\n",
       "      <td>219129.000000</td>\n",
       "      <td>219129.000000</td>\n",
       "      <td>219129.000000</td>\n",
       "      <td>219129.000000</td>\n",
       "      <td>219129.000000</td>\n",
       "      <td>219129.000000</td>\n",
       "      <td>219129.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>219129.000000</td>\n",
       "      <td>219129.000000</td>\n",
       "      <td>219129.000000</td>\n",
       "      <td>219129.000000</td>\n",
       "      <td>219129.000000</td>\n",
       "      <td>219129.000000</td>\n",
       "      <td>219129.000000</td>\n",
       "      <td>219129.000000</td>\n",
       "      <td>219129.000000</td>\n",
       "      <td>219129.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>109564.000000</td>\n",
       "      <td>62377.415376</td>\n",
       "      <td>0.096008</td>\n",
       "      <td>0.048345</td>\n",
       "      <td>0.592102</td>\n",
       "      <td>0.069273</td>\n",
       "      <td>-0.161555</td>\n",
       "      <td>0.133688</td>\n",
       "      <td>-0.128224</td>\n",
       "      <td>0.149534</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031064</td>\n",
       "      <td>-0.050852</td>\n",
       "      <td>-0.050531</td>\n",
       "      <td>-0.002992</td>\n",
       "      <td>0.124005</td>\n",
       "      <td>0.009881</td>\n",
       "      <td>0.014034</td>\n",
       "      <td>0.017313</td>\n",
       "      <td>66.359803</td>\n",
       "      <td>0.002140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>63257.237906</td>\n",
       "      <td>25620.348569</td>\n",
       "      <td>1.395425</td>\n",
       "      <td>1.159805</td>\n",
       "      <td>1.132884</td>\n",
       "      <td>1.253125</td>\n",
       "      <td>1.069530</td>\n",
       "      <td>1.202411</td>\n",
       "      <td>0.817207</td>\n",
       "      <td>0.716212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.422777</td>\n",
       "      <td>0.597812</td>\n",
       "      <td>0.318175</td>\n",
       "      <td>0.593100</td>\n",
       "      <td>0.406741</td>\n",
       "      <td>0.473867</td>\n",
       "      <td>0.233355</td>\n",
       "      <td>0.164859</td>\n",
       "      <td>150.795017</td>\n",
       "      <td>0.046214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-29.807725</td>\n",
       "      <td>-44.247914</td>\n",
       "      <td>-19.722872</td>\n",
       "      <td>-5.263650</td>\n",
       "      <td>-37.591259</td>\n",
       "      <td>-25.659750</td>\n",
       "      <td>-31.179799</td>\n",
       "      <td>-28.903442</td>\n",
       "      <td>...</td>\n",
       "      <td>-14.689621</td>\n",
       "      <td>-8.748979</td>\n",
       "      <td>-11.958588</td>\n",
       "      <td>-2.836285</td>\n",
       "      <td>-3.958591</td>\n",
       "      <td>-1.858672</td>\n",
       "      <td>-9.234767</td>\n",
       "      <td>-4.551680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54782.000000</td>\n",
       "      <td>47933.000000</td>\n",
       "      <td>-0.846135</td>\n",
       "      <td>-0.573728</td>\n",
       "      <td>-0.027154</td>\n",
       "      <td>-0.769256</td>\n",
       "      <td>-0.847346</td>\n",
       "      <td>-0.631835</td>\n",
       "      <td>-0.646730</td>\n",
       "      <td>-0.095948</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.190418</td>\n",
       "      <td>-0.473099</td>\n",
       "      <td>-0.174478</td>\n",
       "      <td>-0.332540</td>\n",
       "      <td>-0.126080</td>\n",
       "      <td>-0.318330</td>\n",
       "      <td>-0.050983</td>\n",
       "      <td>-0.009512</td>\n",
       "      <td>5.990000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>109564.000000</td>\n",
       "      <td>63189.000000</td>\n",
       "      <td>0.385913</td>\n",
       "      <td>0.046937</td>\n",
       "      <td>0.735895</td>\n",
       "      <td>0.064856</td>\n",
       "      <td>-0.229929</td>\n",
       "      <td>-0.087778</td>\n",
       "      <td>-0.098970</td>\n",
       "      <td>0.111219</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042858</td>\n",
       "      <td>-0.032856</td>\n",
       "      <td>-0.063307</td>\n",
       "      <td>0.038708</td>\n",
       "      <td>0.145934</td>\n",
       "      <td>-0.086388</td>\n",
       "      <td>0.015905</td>\n",
       "      <td>0.022163</td>\n",
       "      <td>21.900000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>164346.000000</td>\n",
       "      <td>77519.000000</td>\n",
       "      <td>1.190661</td>\n",
       "      <td>0.814145</td>\n",
       "      <td>1.306110</td>\n",
       "      <td>0.919353</td>\n",
       "      <td>0.356856</td>\n",
       "      <td>0.482388</td>\n",
       "      <td>0.385567</td>\n",
       "      <td>0.390976</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109187</td>\n",
       "      <td>0.354910</td>\n",
       "      <td>0.060221</td>\n",
       "      <td>0.394566</td>\n",
       "      <td>0.402926</td>\n",
       "      <td>0.253869</td>\n",
       "      <td>0.076814</td>\n",
       "      <td>0.066987</td>\n",
       "      <td>68.930000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>219128.000000</td>\n",
       "      <td>120580.000000</td>\n",
       "      <td>2.430494</td>\n",
       "      <td>16.068473</td>\n",
       "      <td>6.145578</td>\n",
       "      <td>12.547997</td>\n",
       "      <td>34.581260</td>\n",
       "      <td>16.233967</td>\n",
       "      <td>39.824099</td>\n",
       "      <td>18.270586</td>\n",
       "      <td>...</td>\n",
       "      <td>22.062945</td>\n",
       "      <td>6.163541</td>\n",
       "      <td>12.734391</td>\n",
       "      <td>4.572739</td>\n",
       "      <td>3.111624</td>\n",
       "      <td>3.402344</td>\n",
       "      <td>13.123618</td>\n",
       "      <td>23.263746</td>\n",
       "      <td>7475.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id           Time             V1             V2  \\\n",
       "count  219129.000000  219129.000000  219129.000000  219129.000000   \n",
       "mean   109564.000000   62377.415376       0.096008       0.048345   \n",
       "std     63257.237906   25620.348569       1.395425       1.159805   \n",
       "min         0.000000       0.000000     -29.807725     -44.247914   \n",
       "25%     54782.000000   47933.000000      -0.846135      -0.573728   \n",
       "50%    109564.000000   63189.000000       0.385913       0.046937   \n",
       "75%    164346.000000   77519.000000       1.190661       0.814145   \n",
       "max    219128.000000  120580.000000       2.430494      16.068473   \n",
       "\n",
       "                  V3             V4             V5             V6  \\\n",
       "count  219129.000000  219129.000000  219129.000000  219129.000000   \n",
       "mean        0.592102       0.069273      -0.161555       0.133688   \n",
       "std         1.132884       1.253125       1.069530       1.202411   \n",
       "min       -19.722872      -5.263650     -37.591259     -25.659750   \n",
       "25%        -0.027154      -0.769256      -0.847346      -0.631835   \n",
       "50%         0.735895       0.064856      -0.229929      -0.087778   \n",
       "75%         1.306110       0.919353       0.356856       0.482388   \n",
       "max         6.145578      12.547997      34.581260      16.233967   \n",
       "\n",
       "                  V7             V8  ...            V21            V22  \\\n",
       "count  219129.000000  219129.000000  ...  219129.000000  219129.000000   \n",
       "mean       -0.128224       0.149534  ...      -0.031064      -0.050852   \n",
       "std         0.817207       0.716212  ...       0.422777       0.597812   \n",
       "min       -31.179799     -28.903442  ...     -14.689621      -8.748979   \n",
       "25%        -0.646730      -0.095948  ...      -0.190418      -0.473099   \n",
       "50%        -0.098970       0.111219  ...      -0.042858      -0.032856   \n",
       "75%         0.385567       0.390976  ...       0.109187       0.354910   \n",
       "max        39.824099      18.270586  ...      22.062945       6.163541   \n",
       "\n",
       "                 V23            V24            V25            V26  \\\n",
       "count  219129.000000  219129.000000  219129.000000  219129.000000   \n",
       "mean       -0.050531      -0.002992       0.124005       0.009881   \n",
       "std         0.318175       0.593100       0.406741       0.473867   \n",
       "min       -11.958588      -2.836285      -3.958591      -1.858672   \n",
       "25%        -0.174478      -0.332540      -0.126080      -0.318330   \n",
       "50%        -0.063307       0.038708       0.145934      -0.086388   \n",
       "75%         0.060221       0.394566       0.402926       0.253869   \n",
       "max        12.734391       4.572739       3.111624       3.402344   \n",
       "\n",
       "                 V27            V28         Amount          Class  \n",
       "count  219129.000000  219129.000000  219129.000000  219129.000000  \n",
       "mean        0.014034       0.017313      66.359803       0.002140  \n",
       "std         0.233355       0.164859     150.795017       0.046214  \n",
       "min        -9.234767      -4.551680       0.000000       0.000000  \n",
       "25%        -0.050983      -0.009512       5.990000       0.000000  \n",
       "50%         0.015905       0.022163      21.900000       0.000000  \n",
       "75%         0.076814       0.066987      68.930000       0.000000  \n",
       "max        13.123618      23.263746    7475.000000       1.000000  \n",
       "\n",
       "[8 rows x 32 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.info()\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218660\n",
      "469\n"
     ]
    }
   ],
   "source": [
    "print(len(train.loc[train['Class'] == 0]))\n",
    "print(len(train.loc[train['Class'] == 1]))\n",
    "\n",
    "for col in train.columns.unique():\n",
    "    missing = sum(train[col].isna())\n",
    "    if missing !=0:\n",
    "        print(f\"{col} - {missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "for i in range(1,29):\n",
    "    features.append(f\"V{i}\")\n",
    "\n",
    "features = ['Time', 'Amount'] + features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = train[features], train[['Class']]\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, train_size=0.8)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "for feat in features:\n",
    "    X_train[feat] = scaler.fit_transform(X_train[feat].to_numpy().reshape(-1,1))\n",
    "    X_val[feat] = scaler.fit_transform(X_val[feat].to_numpy().reshape(-1,1))\n",
    "    test[feat] = scaler.fit_transform(test[feat].to_numpy().reshape(-1,1))\n",
    "\n",
    "\n",
    "\n",
    "def doPCA(data, n_components):\n",
    "    return PCA(n_components=n_components).fit_transform(data)\n",
    "\n",
    "\n",
    "def doSMOTE(X,Y, random_state=0):\n",
    "    smote = SMOTE(random_state=random_state)\n",
    "    A,B = smote.fit_resample(X, Y)\n",
    "    return (A,B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PCA: 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.62      0.56     43733\n",
      "           1       0.52      0.41      0.46     43733\n",
      "\n",
      "    accuracy                           0.52     87466\n",
      "   macro avg       0.52      0.52      0.51     87466\n",
      "weighted avg       0.52      0.52      0.51     87466\n",
      "\n",
      "\n",
      "PCA: 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.87      0.65     43733\n",
      "           1       0.57      0.17      0.26     43733\n",
      "\n",
      "    accuracy                           0.52     87466\n",
      "   macro avg       0.54      0.52      0.46     87466\n",
      "weighted avg       0.54      0.52      0.46     87466\n",
      "\n",
      "\n",
      "PCA: 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.95      0.66     43733\n",
      "           1       0.55      0.06      0.11     43733\n",
      "\n",
      "    accuracy                           0.51     87466\n",
      "   macro avg       0.53      0.51      0.39     87466\n",
      "weighted avg       0.53      0.51      0.39     87466\n",
      "\n",
      "\n",
      "PCA: 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.97      0.66     43733\n",
      "           1       0.57      0.04      0.08     43733\n",
      "\n",
      "    accuracy                           0.51     87466\n",
      "   macro avg       0.54      0.51      0.37     87466\n",
      "weighted avg       0.54      0.51      0.37     87466\n",
      "\n",
      "\n",
      "PCA: 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.97      0.67     43733\n",
      "           1       0.64      0.05      0.10     43733\n",
      "\n",
      "    accuracy                           0.51     87466\n",
      "   macro avg       0.58      0.51      0.38     87466\n",
      "weighted avg       0.58      0.51      0.38     87466\n",
      "\n",
      "\n",
      "PCA: 6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.96      0.66     43733\n",
      "           1       0.55      0.04      0.08     43733\n",
      "\n",
      "    accuracy                           0.50     87466\n",
      "   macro avg       0.52      0.50      0.37     87466\n",
      "weighted avg       0.52      0.50      0.37     87466\n",
      "\n",
      "\n",
      "PCA: 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.97      0.66     43733\n",
      "           1       0.62      0.05      0.09     43733\n",
      "\n",
      "    accuracy                           0.51     87466\n",
      "   macro avg       0.56      0.51      0.38     87466\n",
      "weighted avg       0.56      0.51      0.38     87466\n",
      "\n",
      "\n",
      "PCA: 8\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.97      0.66     43733\n",
      "           1       0.61      0.05      0.09     43733\n",
      "\n",
      "    accuracy                           0.51     87466\n",
      "   macro avg       0.56      0.51      0.38     87466\n",
      "weighted avg       0.56      0.51      0.38     87466\n",
      "\n",
      "\n",
      "PCA: 9\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.96      0.66     43733\n",
      "           1       0.61      0.06      0.10     43733\n",
      "\n",
      "    accuracy                           0.51     87466\n",
      "   macro avg       0.56      0.51      0.38     87466\n",
      "weighted avg       0.56      0.51      0.38     87466\n",
      "\n",
      "\n",
      "PCA: 10\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.97      0.66     43733\n",
      "           1       0.63      0.05      0.09     43733\n",
      "\n",
      "    accuracy                           0.51     87466\n",
      "   macro avg       0.57      0.51      0.38     87466\n",
      "weighted avg       0.57      0.51      0.38     87466\n",
      "\n",
      "\n",
      "PCA: 11\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.97      0.67     43733\n",
      "           1       0.72      0.08      0.14     43733\n",
      "\n",
      "    accuracy                           0.52     87466\n",
      "   macro avg       0.62      0.52      0.41     87466\n",
      "weighted avg       0.62      0.52      0.41     87466\n",
      "\n",
      "\n",
      "PCA: 12\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.97      0.66     43733\n",
      "           1       0.63      0.06      0.11     43733\n",
      "\n",
      "    accuracy                           0.51     87466\n",
      "   macro avg       0.57      0.51      0.39     87466\n",
      "weighted avg       0.57      0.51      0.39     87466\n",
      "\n",
      "\n",
      "PCA: 13\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.96      0.67     43733\n",
      "           1       0.70      0.10      0.17     43733\n",
      "\n",
      "    accuracy                           0.53     87466\n",
      "   macro avg       0.61      0.53      0.42     87466\n",
      "weighted avg       0.61      0.53      0.42     87466\n",
      "\n",
      "\n",
      "PCA: 14\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.96      0.67     43733\n",
      "           1       0.71      0.10      0.17     43733\n",
      "\n",
      "    accuracy                           0.53     87466\n",
      "   macro avg       0.61      0.53      0.42     87466\n",
      "weighted avg       0.61      0.53      0.42     87466\n",
      "\n",
      "\n",
      "PCA: 15\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.97      0.67     43733\n",
      "           1       0.67      0.07      0.13     43733\n",
      "\n",
      "    accuracy                           0.52     87466\n",
      "   macro avg       0.59      0.52      0.40     87466\n",
      "weighted avg       0.59      0.52      0.40     87466\n",
      "\n",
      "\n",
      "PCA: 16\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.96      0.66     43733\n",
      "           1       0.61      0.06      0.11     43733\n",
      "\n",
      "    accuracy                           0.51     87466\n",
      "   macro avg       0.56      0.51      0.39     87466\n",
      "weighted avg       0.56      0.51      0.39     87466\n",
      "\n",
      "\n",
      "PCA: 17\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.97      0.66     43733\n",
      "           1       0.58      0.05      0.09     43733\n",
      "\n",
      "    accuracy                           0.51     87466\n",
      "   macro avg       0.54      0.51      0.37     87466\n",
      "weighted avg       0.54      0.51      0.37     87466\n",
      "\n",
      "\n",
      "PCA: 18\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.96      0.68     43733\n",
      "           1       0.75      0.11      0.19     43733\n",
      "\n",
      "    accuracy                           0.54     87466\n",
      "   macro avg       0.64      0.54      0.43     87466\n",
      "weighted avg       0.64      0.54      0.43     87466\n",
      "\n",
      "\n",
      "PCA: 19\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.97      0.66     43733\n",
      "           1       0.55      0.04      0.07     43733\n",
      "\n",
      "    accuracy                           0.50     87466\n",
      "   macro avg       0.52      0.50      0.37     87466\n",
      "weighted avg       0.52      0.50      0.37     87466\n",
      "\n",
      "\n",
      "PCA: 20\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.96      0.66     43733\n",
      "           1       0.54      0.05      0.09     43733\n",
      "\n",
      "    accuracy                           0.50     87466\n",
      "   macro avg       0.52      0.50      0.37     87466\n",
      "weighted avg       0.52      0.50      0.37     87466\n",
      "\n",
      "\n",
      "PCA: 21\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.96      0.66     43733\n",
      "           1       0.60      0.06      0.11     43733\n",
      "\n",
      "    accuracy                           0.51     87466\n",
      "   macro avg       0.55      0.51      0.38     87466\n",
      "weighted avg       0.55      0.51      0.38     87466\n",
      "\n",
      "\n",
      "PCA: 22\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.96      0.66     43733\n",
      "           1       0.63      0.06      0.12     43733\n",
      "\n",
      "    accuracy                           0.51     87466\n",
      "   macro avg       0.57      0.51      0.39     87466\n",
      "weighted avg       0.57      0.51      0.39     87466\n",
      "\n",
      "\n",
      "PCA: 23\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.96      0.67     43733\n",
      "           1       0.65      0.07      0.13     43733\n",
      "\n",
      "    accuracy                           0.52     87466\n",
      "   macro avg       0.58      0.52      0.40     87466\n",
      "weighted avg       0.58      0.52      0.40     87466\n",
      "\n",
      "\n",
      "PCA: 24\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.96      0.66     43733\n",
      "           1       0.51      0.04      0.07     43733\n",
      "\n",
      "    accuracy                           0.50     87466\n",
      "   macro avg       0.51      0.50      0.36     87466\n",
      "weighted avg       0.51      0.50      0.36     87466\n",
      "\n",
      "\n",
      "PCA: 25\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.96      0.66     43733\n",
      "           1       0.41      0.03      0.05     43733\n",
      "\n",
      "    accuracy                           0.49     87466\n",
      "   macro avg       0.45      0.49      0.35     87466\n",
      "weighted avg       0.45      0.49      0.35     87466\n",
      "\n",
      "\n",
      "PCA: 26\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.95      0.66     43733\n",
      "           1       0.54      0.05      0.10     43733\n",
      "\n",
      "    accuracy                           0.50     87466\n",
      "   macro avg       0.52      0.50      0.38     87466\n",
      "weighted avg       0.52      0.50      0.38     87466\n",
      "\n",
      "\n",
      "PCA: 27\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.96      0.66     43733\n",
      "           1       0.63      0.08      0.13     43733\n",
      "\n",
      "    accuracy                           0.52     87466\n",
      "   macro avg       0.57      0.52      0.40     87466\n",
      "weighted avg       0.57      0.52      0.40     87466\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 28):\n",
    "    x_train, y_train = X_train, Y_train\n",
    "    x_val, y_val = X_val, Y_val\n",
    "\n",
    "    x_train_pca = doPCA(x_train, n_components=i)\n",
    "    x_val_pca = doPCA(x_val, n_components=i)\n",
    "\n",
    "    x_t, y_t = doSMOTE(x_train_pca, y_train)\n",
    "    x_v, y_v = doSMOTE(x_val_pca, y_val)\n",
    "\n",
    "    dt = DecisionTreeClassifier()\n",
    "\n",
    "    dt.fit(x_t, y_t)\n",
    "\n",
    "    y_p = dt.predict(x_v)\n",
    "\n",
    "    print(f\"\\nPCA: {i}\")\n",
    "    print(classification_report(y_v, y_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t = doPCA(X_train, n_components=18)\n",
    "\n",
    "x_t, y_t = doSMOTE(x_t, Y_train)\n",
    "\n",
    "x_ = doPCA(test[features], 18)\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "dt.fit(x_t, y_t)\n",
    "\n",
    "y_p = dt.predict(x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146087\n",
      "4688\n"
     ]
    }
   ],
   "source": [
    "y_pp = pd.DataFrame()\n",
    "y_pp['id'] = test['id']\n",
    "y_pp['Class'] = y_p\n",
    "y_pp.to_csv('attempt1.csv', index=False)\n",
    "\n",
    "print(len(y_pp))\n",
    "print(sum(y_p))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
